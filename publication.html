<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Mark Otto, Jacob Thornton, and Bootstrap contributors">
    <meta name="generator" content="Hugo 0.101.0">
    <title>OpenEarthMap</title>

    <link rel="canonical" href="https://getbootstrap.com/docs/5.2/examples/carousel/">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    

    

<link href="./assets/dist/css/bootstrap.min.css" rel="stylesheet">

    <style>
      .bd-placeholder-img {
        font-size: 1.125rem;
        text-anchor: middle;
        -webkit-user-select: none;
        -moz-user-select: none;
        user-select: none;
      }

      @media (min-width: 768px) {
        .bd-placeholder-img-lg {
          font-size: 3.5rem;
        }
      }

      .b-example-divider {
        height: 3rem;
        background-color: rgba(0, 0, 0, .1);
        border: solid rgba(0, 0, 0, .15);
        border-width: 1px 0;
        box-shadow: inset 0 .5em 1.5em rgba(0, 0, 0, .1), inset 0 .125em .5em rgba(0, 0, 0, .15);
      }

      .b-example-vr {
        flex-shrink: 0;
        width: 1.5rem;
        height: 100vh;
      }

      .bi {
        vertical-align: -.125em;
        fill: currentColor;
      }

      .nav-scroller {
        position: relative;
        z-index: 2;
        height: 2.75rem;
        overflow-y: hidden;
      }

      .nav-scroller .nav {
        display: flex;
        flex-wrap: nowrap;
        padding-bottom: 1rem;
        margin-top: -1px;
        overflow-x: auto;
        text-align: center;
        white-space: nowrap;
        -webkit-overflow-scrolling: touch;
      }
      
      /*.navbar-nav .active {
          color: #19B49B !important;
          }*/

    </style>
    
    <!-- Custom styles for this template -->
    <link href="./assets/css/carousel.css" rel="stylesheet">
    
    <!-- for publication -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
	<script type="text/javascript">
	$(document).ready(function() {      $("button").hover(function(){           $(this).css("cursor","pointer");       },function(){           $(this).css("cursor","default");      });      $("p.abst").css("display","none");      $("button.abst_bt").click(function(){           $(this).next().slideToggle(400);      }); });
	</script>
	<script type="text/javascript">
	$(document).ready(function() {
	    $("p.abst").css("display","none");
	    $("a.abst_bt").click(
		function(){
		    $(this).next().slideToggle(400);      }); });
	</script>
	
	<script src="https://kit.fontawesome.com/4082b32f80.js" crossorigin="anonymous"></script>
	
  </head>
  <body>
    
<header>
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
    <div class="container-xl">
      <a class="navbar-brand" href="https://open-earth-map.org/"><img src="./assets/brand/Geoinfo_Logo.png" alt="" height="40"> Geoinformatics Unit</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarCollapse">
        <ul class="navbar-nav me-auto mb-2 mb-md-0">
          <li class="nav-item">
            <a class="nav-link" aria-current="page" href="./index.html">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" aria-current="page" href="./research.html">Research</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" aria-current="page" href="./team.html">Team</a>
          </li>
          <li class="nav-item">
            <a class="nav-link active" aria-current="page" href="./publication.html">Publication</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" aria-current="page" href="./opportunity.html" target="_blank">Opportunity</a>
          </li>

          <!--<li class="nav-item">
            <a class="nav-link" href="">Download</a>
          </li>-->
          <!--<li class="nav-item">
            <a class="nav-link" href="#">News</a>
          </li>
          <li class="nav-item">
            <a class="nav-link">Overview</a>
          </li>
          <li class="nav-item">
            <a class="nav-link">Demo</a>
          </li>
          <li class="nav-item">
            <a class="nav-link">Download</a>
          </li>
          <li class="nav-item">
            <a class="nav-link">Leaderboard</a>
          </li>
        </ul>-->
        <!--<form class="d-flex" role="search">
          <input class="form-control me-2" type="search" placeholder="Search" aria-label="Search">
          <button class="btn btn-outline-success" type="submit">Search</button>
        </form>-->
      </div>
    </div>
  </nav>
</header>

<main>
  <!-- Marketing messaging and featurettes
  ================================================== -->
  <!-- Wrap the rest of the page in another container to center all the content. -->

  <div class="container marketing" style="padding-top: 5em">
  
  <h2>Journal papers</h2>
  <br><br>
		<ol>
		<li>H. Chen, <span class="font700">N. Yokoya</span>, C. Wu and B. Du, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/9984688" >Unsupervised multimodal change detection based on structural relationship graph representation learning</a>,&#8221; <em>IEEE Transactions on Geoscience and Remote Sensing</em>, 2022.<br>
		<a class="file" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9980430"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <!--<a class="file" href="https://github.com/ibanezfd/MAEST" target="_blank"><i class="far fa-file-code"></i> Code </a>&nbsp;&nbsp; --> <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Unsupervised multimodal change detection is a practical and challenging topic that can play an important role in time-sensitive emergency applications. To address the challenge that multimodal remote sensing images cannot be directly compared due to their modal heterogeneity, we take advantage of two types of modality-independent structural relationships in multimodal images. In particular, we present a structural relationship graph representation learning framework for measuring the similarity of the two structural relationships. Firstly, structural graphs are generated from preprocessed multimodal image pairs by means of an object-based image analysis approach. Then, a structural relationship graph convolutional autoencoder (SR-GCAE) is proposed to learn robust and representative features from graphs. Two loss functions aiming at reconstructing vertex information and edge information are presented to make the learned representations applicable for structural relationship similarity measurement. Subsequently, the similarity levels of two structural relationships are calculated from learned graph representations and two difference images are generated based on the similarity levels. After obtaining the difference images, an adaptive fusion strategy is presented to fuse the two difference images. Finally, a morphological filtering-based postprocessing approach is employed to refine the detection results. Experimental results on six datasets with different modal combinations demonstrate the effectiveness of the proposed method.</p>
		    </li><br>
		<li>X. Ding, J. Kang, Z. Zhang, Y. Huang, J. Liu, and <span class="font700">N. Yokoya</span>, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/9980430" >Coherence-guided complex convolutional sparse coding for interferometric phase restoration</a>,&#8221; <em>IEEE Transactions on Geoscience and Remote Sensing</em>, 2022.<br>
		<a class="file" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9980430"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <!--<a class="file" href="https://github.com/ibanezfd/MAEST" target="_blank"><i class="far fa-file-code"></i> Code </a>&nbsp;&nbsp; --> <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Interferometric phase restoration is a crucial step in retrieving large-scale geophysical parameters from Synthetic Aperture Radar (SAR) images. Existing noise impacts the accuracy of parameter retrieval as a result of decorrelation effects. Most state-of-the-art filtering methods belong to the group of nonlocal filters. In this paper, we propose a novel convolutional sparse coding method in complex domain with the prior knowledge of coherence integrated into the optimization model, which is termed as CoComCSC. CoComCSC is not only capable of reducing noise in regions with continuous phase changes, but also of preserving the phase details prominently. The experiments results on simulated and real data demonstrate the effectiveness of CoComCSC by comparing with other state-of-the-art methods. Moreover, the obtained Digital Elevation Model (DEM) product by CoComCSC from RADARSAT-2 data indicates its superior filtering performance over regions with heterogeneous land-covers, which shows its great potential for generating high-resolution DEM products.</p>
		    </li><br>
		<li>D. Ibañez, R. Fernandez-Beltran, F. Pla, and <span class="font700">N. Yokoya</span>, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/abstract/document/9931741" >Masked auto-encoding spectral-spatial transformer for hyperspectral image classification</a>,&#8221; <em>IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 60, pp. 1-14, 2022.<br>
		<a class="file" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9931741"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="file" href="https://github.com/ibanezfd/MAEST" target="_blank"><i class="far fa-file-code"></i> Code </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Deep learning has certainly become the dominant trend in hyper-spectral (HS) remote sensing image classification owing to its excellent capabilities to extract highly discriminating spectral-spatial features. In this context, transformer networks have recently shown prominent results in distinguishing even the most subtle spectral differences because of their potential to characterize sequential spectral data. Nonetheless, many complexities affecting HS remote sensing data (e.g. atmospheric effects, thermal noise, quantization noise, etc.) may severely undermine such potential since no mode of relieving noisy feature patterns has still been developed within transformer networks. To address the problem, this paper presents a novel masked auto-encoding spectral-spatial transformer (MAEST), which gathers two different collaborative branches: (i) a reconstruction path, which dynamically uncovers the most robust encoding features based on a masking auto-encoding strategy; and (ii) a classification path, which embeds these features onto a transformer network to classify the data focusing on the features that better reconstruct the input. Unlike other existing models, this novel design pursues to learn refined transformer features considering the aforementioned complexities of the HS remote sensing image domain. The experimental comparison, including several state-of-the-art methods and benchmark datasets, shows the superior results obtained by MAEST. The codes of this paper will be available at https://github.com/ibanezfd/MAEST.</p>
		    </li><br>
		<li>T. Xu, T.Z. Huang, L.J. Deng, and <span class="font700">N. Yokoya</span>, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/abstract/document/9777947" target="_blank">An iterative regularization method based on tensor subspace representation for hyperspectral image super-resolution</a>,&#8221; <em>IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 60, pp. 1-16, 2022.<br>
	<a class="file" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9777947" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <a class="file" href="https://github.com/liangjiandeng/IR_TenSR" target="_blank"><i class="far fa-file-code"></i> Code </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Hyperspectral image super-resolution (HSI-SR) can be achieved by fusing a paired multispectral image (MSI) and hyperspectral image (HSI), which is a prevalent strategy. But, how to precisely reconstruct the high spatial resolution hyperspectral image (HR-HSI) by fusion technology is a challenging issue. In this article, we propose an iterative regularization method based on tensor subspace representation (IR-TenSR) for MSI-HSI fusion, thus HSI-SR. First, we propose a tensor subspace representation (TenSR)-based regularization model that integrates the global spectral–spatial low-rank and the nonlocal self-similarity priors of HR-HSI. These two priors have been proven effective, but previous HSI-SR works cannot simultaneously exploit them. Subsequently, we design an iterative regularization procedure to utilize the residual information of acquired low-resolution images, which are ignored in other works that produce suboptimal results. Finally, we develop an effective algorithm based on the proximal alternating minimization method to solve the TenSR-regularization model. With that, we obtain the iterative regularization algorithm. Experiments implemented on the simulated and real datasets illustrate the advantages of the proposed IR-TenSR compared with the state-of-the-art fusion approaches. The code is available at https://github.com/liangjiandeng/IR_TenSR.</p>
		    </li><br>
		<li>D. Ibañez, R. Fernandez-Beltran, F. Pla, and <span class="font700">N. Yokoya</span>, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/9739954" target="_blank">DAT-CNN: Dual attention temporal CNN for time-resolving Sentinel-3 vegetation indices</a>,&#8221; <em>IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.</em>, vol. 15, pp. 2632-2643, 2022.<br>
	<a class="file" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9739954" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <a class="file" href="https://github.com/ibanezfd/DATCNN" target="_blank"><i class="far fa-file-code"></i> Code </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> The synergies between Sentinel-3 (S3) and the forthcoming fluorescence explorer (FLEX) mission bring us the opportunity of using S3 vegetation indices (VI) as proxies of the solar-induced chlorophyll fluorescence (SIF) that will be captured by FLEX. However, the highly dynamic nature of SIF demands a very temporally accurate monitoring of S3 VIs to become reliable proxies. In this scenario, this article proposes a novel temporal reconstruction convolutional neural network (CNN), named dual attention temporal CNN (DAT-CNN), which has been specially designed for time-resolving S3 VIs using S2 and S3 multitemporal observations. In contrast to other existing techniques, DAT-CNN implements two different branches for processing and fusing S2 and S3 multimodal data, while further exploiting intersensor synergies. Besides, DAT-CNN also incorporates a new spatial–spectral and temporal attention module to suppress uninformative spatial–spectral features, while focusing on the most relevant temporal stamps for each particular prediction. The experimental comparison, including several temporal reconstruction methods and multiple operational Sentinel data products, demonstrates the competitive advantages of the proposed model with respect to the state of the art. The codes of this article will be available at https://github.com/ibanezfd/DATCNN .</p>
		    </li><br>
		<li>J. Xia, <span class="font700">N. Yokoya</span>, and G. Baier, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/9701349">DML: Differ-modality learning for building semantic segmentation</a>,&#8221; <em>IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 60, pp. 1-14, 2022.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> This work critically analyzes the problems arising from differ-modality building semantic segmentation in the remote sensing domain. With the growth of multi-modality datasets, such as optical, synthetic aperture radar (SAR), light detection and ranging (LiDAR), and the scarcity of semantic knowledge, the task of learning multi-modality information has increasingly become relevant over the last few years. However, multi-modality datasets cannot be obtained simultaneously due to many factors. Assume we have SAR images with reference information in one place and optical images without reference in another; how to learn relevant features of optical images from SAR images? We refer to it as differ-modality learning (DML). To solve the DML problem, we propose novel deep neural network architectures, which include image adaptation, feature adaptation, knowledge distillation, and self-training modules for different scenarios.  We test the proposed methods on the differ-modality remote sensing datasets (very high-resolution SAR and RGB from SpaceNet 6) to build semantic segmentation and achieve superior efficiency. The presented approach achieves the best performance when compared to the state-of-the-art methods.</p>
		    </li><br>
		<li>Z. Li, F. Lu, H. Zhang, L. Tu, J. Li, X. Huang, C. Robinson, N. Malkin, N. Jojic, P. Ghamisi, R. Hänsch, and, <span class="font700">N. Yokoya</span>, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/9690575">The Outcome of the 2021 IEEE GRSS Data Fusion Contest - Track MSD: Multitemporal semantic change detection</a>,&#8221; <em>IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.</em>, 2022.<br>
	<a class="file" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9690575"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> We present here the scientific outcomes of the 2021 Data Fusion Contest (DFC2021) organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. DFC2021 was dedicated to research on geospatial artificial intelligence (AI) for social good with a global objective of modeling the state and changes of artificial and natural environments from multimodal and multitemporal remotely sensed data towards sustainable developments. DFC2021 included two challenge tracks: ``Detection of settlements without electricity'' and ``Multitemporal semantic change detection''. This paper mainly focuses on the outcome of the multitemporal semantic change detection track. We describe in this paper the DFC2021 dataset that remains available for further evaluation of corresponding approaches and report the results of the best-performing methods during the contest.</p>
		    </li><br>
		<li>Y. Ma, Y. Li, K. Feng, Y. Xia, Q. Huang, H. Zhang, C. Prieur, G. Licciardi, H. Malha, J. Chanussot, P. Ghamisi, R. Hänsch, and, <span class="font700">N. Yokoya</span>, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/9626564">The Outcome of the 2021 IEEE GRSS Data Fusion Contest - Track DSE: Detection of settlements without electricity</a>,&#8221; <em>IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.</em>, vol. 14, pp. 12375-12385, 2021.<br>
	<a class="file" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9626564"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> In this article, we elaborate on the scientific outcomes of the 2021 Data Fusion Contest (DFC2021), which was organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society, on the subject of geospatial artificial intelligence for social good. The ultimate objective of the contest was to model the state and changes of artificial and natural environments from multimodal and multitemporal remotely sensed data towards sustainable developments. DFC2021 consisted of two challenge tracks: Detection of settlements without electricity (DSE) and multitemporal semantic change detection. We focus here on the outcome of the DSE track. This article presents the corresponding approaches and reports the results of the best-performing methods during the contest.</p>
		    </li><br>
		<li>X. Sun, P. Wang, Z. Yan, W. Diao, X. Lu, Z. Yang, Y. Zhang, D. Xiang, C. Yan, J. Guo, B. Dang, W. Wei, F. Xu, C. Wang, R. Hansch, M. Weinmann, <span class="font700">N. Yokoya</span>, and K. Fu, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/abstract/document/9521691">Automated high-resolution earth observation image interpretation: Outcome of the 2020 Gaofen Challenge</a>,&#8221; <em>IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.</em>, 2021.<br>
	<a class="file" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9521691"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> In this paper, we introduce the 2020 Gaofen Challenge and relevant scientific outcomes. The 2020 Gaofen Challenge is an international competition, which is organized by the China High-Resolution Earth Observation Conference Committee and the Aerospace Information Research Institute, Chinese Academy of Sciences and technically co-sponsored by the IEEE Geoscience and Remote Sensing Society (IEEE-GRSS) and the International Society for Photogrammetry and Remote Sensing (ISPRS). It aims at promoting the academic development of automated high-resolution earth observation image interpretation. Six independent tracks have been organized in this challenge, which cover the challenging problems in the field of object detection and semantic segmentation. With the development of convolutional neural networks, deep learning-based methods have achieved good performance on image interpretation. In this paper, we report the details and the best-performing methods presented so far in the scope of this challenge.</p>
		    </li><br>
		<li>W. He, Y. Chen, <span class="font700">N. Yokoya</span>, C. Li, and Q. Zhao, &#8221;<a class="paper" href="https://arxiv.org/abs/2001.01547">Hyperspectral super-resolution via coupled tensor ring factorization</a>,&#8221;	<em>Pattern Recognition</em>, 2021.<br>
	<a class="file" href="https://arxiv.org/pdf/2001.01547"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Hyperspectral super-resolution (HSR) fuses a low-resolution hyperspectral image (HSI) and a high-resolution multispectral image (MSI) to obtain a high-resolution HSI (HR-HSI). In this paper, we propose a new model, named coupled tensor ring factorization (CTRF), for HSR. The proposed CTRF approach simultaneously learns high spectral resolution core tensor from the HSI and high spatial resolution core tensors from the MSI, and reconstructs the HR-HSI via tensor ring (TR) representation (Figure 1). The CTRF model can separately exploit the low-rank property of each class (Section III-C), which has been never explored in the previous coupled tensor model. Meanwhile, it inherits the simple representation of coupled matrix/CP factorization and flexible low-rank exploration of coupled Tucker factorization. Guided by Theorem 2, we further propose a spectral nuclear norm regularization to explore the global spectral low-rank property. The experiments have demonstrated the advantage of the proposed nuclear norm regularized CTRF (NCTRF) as compared to previous matrix/tensor and deep learning methods.</p>
		    </li><br>
		<li>W. He, <span class="font700">N. Yokoya</span>, X. Yuan, &#8221;<a class="paper" href="https://arxiv.org/abs/2012.15104">Fast hyperspectral image recovery via non-iterative fusion of dual-camera compressive hyperspectral imaging</a>,&#8221; <em>IEEE Transactions on Image Processing</em>, (accepted for publication), 2021.<br>
	<a class="file" href="https://arxiv.org/pdf/2012.15104"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Coded aperture snapshot spectral imaging (CASSI) is a promising technique to capture the three-dimensional hyperspectral image (HSI) using a single coded two-dimensional (2D) measurement, in which algorithms are used to perform the inverse problem. Due to the ill-posed nature, various regularizers have been exploited to reconstruct the 3D data from the 2D measurement. Unfortunately, the accuracy and computational complexity are unsatisfied. One feasible solution is to utilize additional information such as the RGB measurement in CASSI. Considering the combined CASSI and RGB measurement, in this paper, we propose a new fusion model for the HSI reconstruction. We investigate the spectral low-rank property of HSI composed of a spectral basis and spatial coefficients. Specifically, the RGB measurement is utilized to estimate the coefficients, meanwhile the CASSI measurement is adopted to provide the orthogonal spectral basis. We further propose a patch processing strategy to enhance the spectral low-rank property of HSI. The proposed model neither requires non-local processing or iteration, nor the spectral sensing matrix of the RGB detector. Extensive experiments on both simulated and real HSI dataset demonstrate that our proposed method outperforms previous state-of-the-art not only in quality but also speeds up the reconstruction more than 5000 times.</p>
		    </li><br>
		<li>N. Le, T. D. Pham, <span class="font700">N. Yokoya</span>, and H. N. Thang, &#8221;<a class="paper" href="">Learning from multimodal and multisensory earth observation dataset for improving estimates of mangrove soil organic carbon in Vietnam</a>,&#8221; <em>International Journal of Remote Sensing</em>, (in press), 2021.<br>
	<!--<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> </p>-->
		    </li><br>
		<li>J. Xia, <span class="font700">N. Yokoya</span>, B. Adriano, L. Zhang, G. Li, Z. Wang, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/9444638">A benchmark high-resolution GaoFen-3 SAR dataset for building semantic segmentation</a>,&#8221; <em>IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.</em>, 2021.<br>
	<a class="file" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9444638"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Deep learning is increasingly popular in remote sensing communities and already successful in land cover classification and semantic segmentation. However, most studies are limited to the utilization of optical datasets. Despite few attempts applied to synthetic aperture radar (SAR) using deep learning, the huge potential, especially for the very high-resolution SAR, are still underexploited. Taking building segmentation as an example, the very high resolution (VHR) SAR datasets are still missing to the best of our knowledge. A comparable baseline for SAR building segmentation does not exist, and which segmentation method is more suitable for SAR image is poorly understood. This paper first provides a benchmark high-resolution (1 m) GaoFen-3 SAR datasets, which cover nine cities from seven countries, review the state-of-the-art semantic segmentation methods applied to SAR, and then summarize the potential operations to improve the performance. With these comprehensive assessments, we hope to provide the recommendation and roadmap for future SAR semantic segmentation.</p>
		    </li><br>
		<li>D. Hong, L. Gao, J. Yao, <span class="font700">N. Yokoya</span>, J. Chanussot, U. Heiden, B. Zhang, &#8221;<a class="paper" href="">Endmember-guided unmixing network (EGU-Net): A general deep learning framework for self-supervised hyperspectral unmixing</a>,&#8221; <em>IEEE Transactions on Neural Networks and Learning Systems</em>, (in press), 2021.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <a class="file" href="https://github.com/danfenghong/IEEE_TNNLS_EGU-Net" target="_blank"><i class="far fa-file-code"></i> Code </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Over the past decades, enormous efforts have been made to improve the performance of linear or nonlinear mixing models for hyperspectral unmixing, yet their ability to simultaneously generalize various spectral variabilities and extract physically meaningful endmembers still remains limited due to the poor ability in data fitting and reconstruction and the sensitivity to various spectral variabilities. Inspired by the powerful learning ability of deep learning, we attempt to develop a general deep learning approach for hyperspectral unmixing, by fully considering the properties of endmembers extracted from the hyperspectral imagery, called endmember-guided unmixing network (EGU-Net). Beyond the alone autoencoder-like architecture, EGU-Net is a two-stream Siamese deep network, which learns an additional network from the pure or nearly-pure endmembers to correct the weights of another unmixing network by sharing network parameters and adding spectrally meaningful constraints (e.g., non-negativity and sum-to-one) towards a more accurate and interpretable unmixing solution. Furthermore, the resulting general framework is not only limited to pixel-wise spectral unmixing but also applicable to spatial information modeling with convolutional operators for spatial-spectral unmixing. Experimental results conducted on three different datasets with the ground-truth of abundance maps corresponding to each material demonstrate the effectiveness and superiority of the EGU-Net over state-of-the-art unmixing algorithms. The codes will be available from the website: <a href="https://github.com/danfenghong/IEEE_TNNLS_EGU-Net" target="_blank">https://github.com/danfenghong/IEEE_TNNLS_EGU-Net</a>.</p>
		    </li><br>
		<li>Y. Qu, H. Qi, C. Kwan, <span class="font700">N. Yokoya</span>, J. Chanussot, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/abstract/document/9442804">Unsupervised and unregistered hyperspectral image super-resolution with mutual dirichlet-net</a>,&#8221; <em>IEEE Transactions on Geoscience and Remote Sensing</em>, (in press), 2021.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Hyperspectral images (HSI) provide rich spectral information that has contributed to the successful performance improvement of numerous computer vision and remote sensing tasks. However, it can only be achieved at the expense of images’ spatial resolution. Hyperspectral image super-resolution (HSI- SR) thus addresses this problem by fusing low resolution (LR) HSI with multispectral image (MSI) carrying much higher spatial resolution (HR). Existing HSI-SR approaches require the LR HSI and HR MSI to be well registered and the reconstruction accuracy of the HR HSI relies heavily on the registration accuracy of different modalities. In this paper, we propose an unregistered and unsupervised mutual Dirichlet-Net (u2-MDN) to exploit the uncharted problem domain of HSI-SR without the requirement of multi-modality registration. The success of this endeavor would largely facilitate the deployment of HSI-SR since registration requirement is difficult to satisfy in real-world sensing devices. The novelty of this work is three-fold. First, to stabilize the fusion procedure of two unregistered modalities, the network is designed to extract spatial and spectral information of two modalities with different dimensions through a shared encoder-decoder structure. Second, the mutual information (MI) is further adopted to capture the non-linear statistical dependen- cies between the representations from two modalities (carrying spatial information) and their raw inputs. By maximizing the MI, spatial correlations between different modalities can be well characterized to further reduce the spectral distortion. We assume the representations follow a similar Dirichlet distribution for its inherent sum-to-one and non-negative properties. Third, a collaborative l2,1 norm is employed as the reconstruction error instead of the more common l2 norm to better preserve the spectral information. Extensive experimental results demonstrate the superior performance of u2-MDN as compared to the state- of-the-art.</p>
		    </li><br>
		<li>G. Baier, A. Deschemps, M. Schmitt, and <span class="font700">N. Yokoya</span>, &#8221;<a class="paper" href="https://arxiv.org/abs/2011.11314">Synthesizing optical and SAR imagery from land cover maps and auxiliary raster data</a>,&#8221; <em>IEEE Transactions on Geoscience and Remote Sensing</em>, (in press), 2021.<br>
	<a class="file" href="https://arxiv.org/pdf/2011.11314"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;  <a class="file" href="https://github.com/gbaier/rs_img_synth" target="_blank"><i class="far fa-file-code"></i> Code </a>&nbsp;&nbsp;  <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> We synthesize both optical RGB and SAR remote sensing images from land cover maps and auxiliary raster data using GANs. In remote sensing many types of data, such as digital elevation models or precipitation maps, are often not reflected in land cover maps but still influence image content or structure. Including such data in the synthesis process increases the quality of the generated images and exerts more control on their characteristics. Spatially adaptive normalization layers fuse both inputs and are applied to a full-blown generator architecture consisting of encoder and decoder, to take full advantage of the information content in the auxiliary raster data. Our method successfully synthesizes medium (10m) and high (1m) resolution images, when trained with the corresponding dataset. We show the advantage of data fusion of land cover maps and auxiliary information using mean intersection over union, pixel accuracy and Fréchet  inception  distance using pre-trained U-Net segmentation models. Handpicked images exemplify how fusing information avoids ambiguities in the synthesized images. By slightly editing the input our method can be used to synthesize realistic changes, i.e., raising the water levels. The source code is available at <a href="https://github.com/gbaier/rs_img_synth" target="_blank">this https URL</a> and we published the newly created high-resolution dataset at <a href="https://ieee-dataport.org/open-access/geonrw" target="_blank">this https URL</a>.</p>
		    </li><br>
		<li>C. Robinson, K. Malkin, N. Jojic, H. Chen, R. Qin, C. Xiao, M. Schmitt, P. Ghamisi, R. Haensch, and <span class="font700">N. Yokoya</span>, &#8221;<a class="paper" href="">Global land cover mapping with weak supervision: Outcome of the 2020 IEEE GRSS Data Fusion Contest</a>,&#8221; <em>IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.</em> (in press), 2021.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> This paper presents the scientific outcomes of the 2020 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2020 Contest addressed the problem of automatic global land-cover mapping with weak supervision, i.e. estimating high-resolution semantic maps while only low-resolution reference data is available during training. Two separate competitions were organized to assess two different scenarios: 1) high-resolution labels are not available at all and 2) a small amount of high-resolution labels are available additionally to low-resolution reference data. In this paper we describe the DFC2020 dataset that remains available for further evaluation of corresponding approaches and report the results of the best-performing methods during the contest.</p>
		    </li><br>
		<li>B. Adriano, <span class="font700">N. Yokoya</span>, J. Xia, H. Miura, W. Liu, M. Matsuoka, S. Koshimura, &#8221;<a class="paper" href="https://arxiv.org/abs/2009.06200">Learning from multimodal and multitemporal earth observation data for building damage mapping</a>,&#8221; <em>ISPRS Journal of Photogrammetry and Remote Sensing</em> (in press), 2021.<br>
	<a class="file" href="https://arxiv.org/pdf/2009.06200"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Earth observation technologies, such as optical imaging and synthetic aperture radar (SAR), provide excellent means to monitor ever-growing urban environments continuously. Notably, in the case of large-scale disasters (e.g., tsunamis and earthquakes), in which a response is highly time-critical, images from both data modalities can complement each other to accurately convey the full damage condition in the disaster's aftermath. However, due to several factors, such as weather and satellite coverage, it is often uncertain which data modality will be the first available for rapid disaster response efforts. Hence, novel methodologies that can utilize all accessible EO datasets are essential for disaster management. In this study, we have developed a global multisensor and multitemporal dataset for building damage mapping. We included building damage characteristics from three disaster types, namely, earthquakes, tsunamis, and typhoons, and considered three building damage categories. The global dataset contains high-resolution optical imagery and high-to-moderate-resolution multiband SAR data acquired before and after each disaster. Using this comprehensive dataset, we analyzed five data modality scenarios for damage mapping: single-mode (optical and SAR datasets), cross-modal (pre-disaster optical and post-disaster SAR datasets), and mode fusion scenarios. We defined a damage mapping framework for the semantic segmentation of damaged buildings based on a deep convolutional neural network algorithm. We compare our approach to another state-of-the-art baseline model for damage mapping. The results indicated that our dataset, together with a deep learning network, enabled acceptable predictions for all the data modality scenarios.</p>
		    </li><br>
		<li>D. Hong, W. He, <span class="font700">N. Yokoya</span>, J. Yao, L. Gao, L. Zhang, J. Chanussot, and X.X. Zhu, &#8221;<a class="paper" href="">Interpretable hyperspectral AI: When non-convex modeling meets hyperspectral remote sensing</a>,&#8221; <em>IEEE Geoscience and Remote Sensing Magazine</em> (in press), 2021.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Hyperspectral imaging, also known as image spectrometry, is a landmark technique in geoscience and remote sensing (RS). In the past decade, enormous efforts have been made to process and analyze these hyperspectral (HS) products mainly by means of seasoned experts. However, with the ever-growing volume of data, the bulk of costs in manpower and material resources poses new challenges on reducing the burden of manual labor and improving efficiency. For this reason, it is, therefore, urgent to develop more intelligent and automatic approaches for various HS RS applications. Machine learning (ML) tools with convex optimization have successfully undertaken the tasks of numerous artificial intelligence (AI)-related applications. However, their ability in handling complex practical problems remains limited, particularly for HS data, due to the effects of various spectral variabilities in the process of HS imaging and the complexity and redundancy of higher dimensional HS signals. Compared to the convex models, non-convex modeling, which is capable of characterizing more complex real scenes and providing the model interpretability technically and theoretically, has been proven to be a feasible solution to reduce the gap between challenging HS vision tasks and currently advanced intelligent data processing models. This article mainly presents an advanced and cutting-edge technical survey for non-convex modeling towards interpretable AI models covering a board scope in the following topics of HS RS: 1) HS image restoration, 2) dimensionality reduction, 3) data fusion and enhancement, 3) spectral unmixing, 4) cross-modality learning for large-scale land cover mapping. Around these topics, we will showcase the significance of non-convex techniques to bridge the gap between HS RS and interpretable AI models with a brief introduction on the research background and motivation, an emphasis on the resulting methodological foundations and solution, and an intuitive clarification of illustrative examples. At the end of each topic, we also pose the remaining challenges on how to completely model the issues of complex spectral vision from the perspective of intelligent ML combined with physical priors and numerical non-convex modeling, and accordingly point out future research directions. This paper aims to create a good entry point to the advanced literature for experienced researchers, Ph.D. students, and engineers who already have some background knowledge in HS RS, ML, and optimization. This can further help them launch new investigations on the basis of the above topics and interpretable AI techniques for their focused fields.</p>
		    </li><br>
		<li>T. D. Pham, <span class="font700">N. Yokoya</span>, T. T. T. Nguyen, N. N. Le, N. T. Ha, J. Xia, W. Takeuchi, T. D. Pham, &#8221;<a class="paper" href="https://www.tandfonline.com/doi/abs/10.1080/15481603.2020.1857623" target="_blank">Improvement of mangrove soil carbon stocks estimation in North Vietnam using Sentinel-2 data and machine learning approach</a>,&#8221; <em>GIScience & Remote Sensing</em>, 2020.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Quantifying total carbon (TC) stocks in soil across various mangrove ecosystems is key to understanding the global carbon cycle to reduce greenhouse gas emissions. Estimating mangrove TC at a large scale remains challenging due to the difficulty and high cost of soil carbon measurements when the number of samples is high. In the present study, we investigated the capability of Sentinel-2 multispectral data together with a state-of-the-art machine learning (ML) technique, which is a combination of CatBoost regression (CBR) and a genetic algorithm (GA) for feature selection and optimization (the CBR-GA model) to estimate the mangrove soil C stocks across the mangrove ecosystems in North Vietnam. We used the field survey data collected from 177 soil cores. We compared the performance of the proposed model with those of the four ML algorithms, i.e., the extreme gradient boosting regression (XGBR), the light gradient boosting machine regression (LGBMR), the support vector regression (SVR), and the random forest regression (RFR) models. Our proposed model estimated the TC level in the soil as 35.06–166.83 Mg ha−1 (average = 92.27 Mg ha−1) with satisfactory accuracy (R 2 = 0.665, RMSE = 18.41 Mg ha−1) and yielded the best prediction performance among all the ML techniques. We conclude that the Sentinel-2 data combined with the CBR-GA model can improve estimates of the mangrove TC at 10 m spatial resolution in tropical areas. The effectiveness of the proposed approach should be further evaluated for different mangrove soils of the other mangrove ecosystems in tropical and semi-tropical regions.</p>
		    </li><br>
		<li>M. Pourshamsi, J. Xia, <span class="font700">N. Yokoya</span>, M. Garcia, M. Lavalle, E. Pottier, and H. Balzter, &#8221;<a class="paper" href="">Tropical forest canopy height estimation from combined polarimetric SAR and LiDAR using machine learning</a>,&#8221; <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, vol. 172, pp. 79-94, 2021.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Forest height is an important forest biophysical parameter which is used to derive important information about forest ecosystems, such as forest above ground biomass. In this paper, the potential of combining Polarimetric Synthetic Aperture Radar (PolSAR) variables with LiDAR measurements for forest height estimation is investigated. This will be conducted using different machine learning algorithms including Random Forest (RFs), Rotation Forest (RoFs), Canonical Correlation Forest (CCFs) and Support Vector Machine (SVMs). Various PolSAR parameters are required as input variables to ensure a successful height retrieval across different forest heights ranges. The algorithms are trained with 5000 LiDAR samples (less than 1% of the full scene) and different polarimetric variables. To examine the dependency of the algorithm on input training samples, three different subsets are identified which each includes different features: subset 1 is quiet diverse and includes non-vegetated region, short/sparse vegetation (0–20 m), vegetation with mid-range height (20–40 m) to tall/dense ones (40–60 m); subset 2 covers mostly the dense vegetated area with height ranges 40–60 m; and subset 3 mostly covers the non-vegetated to short/sparse vegetation (0–20 m) .The trained algorithms were used to estimate the height for the areas outside the identified subset. The results were validated with independent samples of LiDAR-derived height showing high accuracy (with the average R2 = 0.70 and RMSE = 10 m between all the algorithms and different training samples). The results confirm that it is possible to estimate forest canopy height using PolSAR parameters together with a small coverage of LiDAR height as training data.</p>
		    </li><br>
		<li>J. Xia, <span class="font700">N. Yokoya</span>, and T. D. Pham, &#8221;<a class="paper" href="https://www.mdpi.com/2072-4292/12/22/3834">Probabilistic mangrove species mapping with multiple-source remote-sensing datasets using label distribution learning in Xuan Thuy National Park, Vietnam</a>,&#8221; <em>Remote Sensing</em>, vol. 12, no. 22, p. 3834, 2020.<br>
	<a class="file" href="https://www.mdpi.com/2072-4292/12/22/3834/pdf"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Mangrove forests play an important role in maintaining water quality, mitigating climate change impacts, and providing a wide range of ecosystem services. Effective identification of mangrove species using remote-sensing images remains a challenge. The combinations of multi-source remote-sensing datasets (with different spectral/spatial resolution) are beneficial to the improvement of mangrove tree species discrimination. In this paper, various combinations of remote-sensing datasets including Sentinel-1 dual-polarimetric synthetic aperture radar (SAR), Sentinel-2 multispectral, and Gaofen-3 full-polarimetric SAR data were used to classify the mangrove communities in Xuan Thuy National Park, Vietnam. The mixture of mangrove communities consisting of small and shrub mangrove patches is generally difficult to separate using low/medium spatial resolution. To alleviate this problem, we propose to use label distribution learning (LDL) to provide the probabilistic mapping of tree species, including Sonneratia caseolaris (SC), Kandelia obovata (KO), Aegiceras corniculatum (AC), Rhizophora stylosa (RS), and Avicennia marina (AM). The experimental results show that the best classification performance was achieved by an integration of Sentinel-2 and Gaofen-3 datasets, demonstrating that full-polarimetric Gaofen-3 data is superior to the dual-polarimetric Sentinel-1 data for mapping mangrove tree species in the tropics.</p>
		    </li><br>
		<li><span class="font700">N. Yokoya</span>, K. Yamanoi, W. He, G. Baier, B. Adriano, H. Miura, and S. Oishi, &#8221;<a class="paper" href="https://arxiv.org/abs/2006.05180">Breaking limits of remote sensing by deep learning from simulated data for flood and debris flow mapping</a>,&#8221;	<em>IEEE Transactions on Geoscience and Remote Sensing</em>, (early access), 2020.<br>
	<a class="file" href="https://arxiv.org/pdf/2006.05180"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="file" href="https://github.com/nyokoya/dlsim" target="_blank"><i class="far fa-file-code"></i> Code </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> We propose a framework that estimates inundation depth (maximum water level) and debris-flow-induced topographic deformation from remote sensing imagery by integrating deep learning and numerical simulation. A water and debris flow simulator generates training data for various artificial disaster scenarios. We show that regression models based on Attention U-Net and LinkNet architectures trained on such synthetic data can predict the maximum water level and topographic deformation from a remote sensing-derived change detection map and a digital elevation model. The proposed framework has an inpainting capability, thus mitigating the false negatives that are inevitable in remote sensing image analysis. Our framework breaks limits of remote sensing and enables rapid estimation of inundation depth and topographic deformation, essential information for emergency response, including rescue and relief activities. We conduct experiments with both synthetic and real data for two disaster events that caused simultaneous flooding and debris flows and demonstrate the effectiveness of our approach quantitatively and qualitatively. Our code and datasets are available at <a href="https://github.com/nyokoya/dlsim" target="_blank">https://github.com/nyokoya/dlsim</a>.</p>
		    </li><br>
		<li>Y. Lian, T. Feng, J. Zhou, M. Jia, A. Li, Z. Wu, L. Jiao, M. Brown, G. Hager, <span class="font700">N. Yokoya</span>, R. Haensch, and B. Le Saux, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/9246669">Large-Scale Semantic 3D Reconstruction: Outcome of the 2019 IEEE GRSS Data Fusion Contest - Part B</a>,&#8221; <em>IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.</em>, (early access), 2020.<br>
	<a class="file" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9246669"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> We present the scientific outcomes of the 2019 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The contest included challenges with large-scale data sets for semantic 3D reconstruction from satellite images and also semantic 3D point cloud classification from airborne LiDAR. 3D reconstruction results are discussed separately in Part-A. In this Part-B, we report the results of the two}best-performing approaches for 3D point cloud classification. Both are deep learning methods that improve upon the PointSIFT model with mechanisms to combine multi-scale features and task-specific post-processing to refine model outputs.</p>
		    </li><br>
		<li>S. Kunwar, H. Chen, M. Lin, H. Zhang, P. D'Angelo, D. Cerra, S. M. Azimi, 
M. Brown, G. Hager, <span class="font700">N. Yokoya</span>, R. Haensch, and B. Le Saux, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/9229514">Large-Scale Semantic 3D Reconstruction: Outcome of the 2019 IEEE GRSS Data Fusion Contest - Part A</a>,&#8221; <em>IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.</em>, (early access), 2020.<br>
	<a class="file" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9229514"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> In this paper, we present the scientific outcomes of the 2019 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2019 Contest addressed the problem of 3D reconstruction and 3D semantic understanding on a large scale. Several competitions were organized to assess specific issues, such as elevation estimation and semantic mapping from a single view, two views, or multiple views. In this Part A, we report the results of the best-performing approaches for semantic 3D reconstruction according to these various set-ups, while 3D point cloud semantic mapping is discussed in Part B.</p>
		    </li><br>
		<li>W. He, Q. Yao, C. Li, <span class="font700">N. Yokoya</span>, Q. Zhao, H. Zhang, L. Zhang, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/9208755">Non-local meets global: An iterative paradigm for hyperspectral image restoration</a>,&#8221; <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, (early access), 2020.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Non-local low-rank tensor approximation has been developed as a state-of-the-art method for hyperspectral image (HSI) restoration, which includes the tasks of denoising, compressed HSI reconstruction and inpainting. Unfortunately, while its restoration performance benefits from more spectral bands, its runtime also substantially increases. In this paper, we claim that the HSI lies in a global spectral low-rank subspace, and the spectral subspaces of each full band patch group should lie in this global low-rank subspace. This motivates us to propose a unified paradigm combining the spatial and spectral properties for HSI restoration. The proposed paradigm enjoys performance superiority from the non-local spatial denoising and light computation complexity from the low-rank orthogonal basis exploration. An efficient alternating minimization algorithm with rank adaptation is developed. It is done by first solving a fidelity term-related problem for the update of a latent input image, and then learning a low-dimensional orthogonal basis and the related reduced image from the latent input image. Subsequently, non-local low-rank denoising is developed to refine the reduced image and orthogonal basis iteratively. Finally, the experiments on HSI denoising, compressed reconstruction, and inpainting tasks, with both simulated and real datasets, demonstrate its superiority with respect to state-of-the-art HSI restoration methods.</p>
		    </li><br>
		<li>D. Hong, J. Kang, <span class="font700">N. Yokoya</span>, and J. Chanussot, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/abstract/document/9204461">Graph-induced aligned learning on subspaces for hyperspectral and multispectral data</a>,&#8221; <em>IEEE Transactions on Geoscience and Remote Sensing</em>, (early access), 2020.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> In this article, we have great interest in investigating a common but practical issue in remote sensing (RS)--can a limited amount of one information-rich (or high-quality) data, e.g., hyperspectral (HS) image, improve the performance of a classification task using a large amount of another information-poor (low-quality) data, e.g., multispectral (MS) image? This question leads to a typical cross-modality feature learning. However, classic cross-modality representation learning approaches, e.g., manifold alignment, remain limited in effectively and efficiently handling such problems that the data from high-quality modality are largely absent. For this reason, we propose a novel graph-induced aligned learning (GiAL) framework by 1) adaptively learning a unified graph (further yielding a Laplacian matrix) from the data in order to align multimodality data (MS-HS data) into a latent shared subspace; 2) simultaneously modeling two regression behaviors with respect to labels and pseudo-labels under a multitask learning paradigm; and 3) dramatically updating the pseudo-labels according to the learned graph and refeeding the latest pseudo-labels into model learning of the next round. In addition, an optimization framework based on the alternating direction method of multipliers (ADMMs) is devised to solve the proposed GiAL model. Extensive experiments are conducted on two MS-HS RS data sets, demonstrating the superiority of the proposed GiAL compared with several state-of-the-art methods.</a>.</p>
		    </li><br>
		<li>D. Hong, <span class="font700">N. Yokoya</span>, J. Chanussot, J. Xu, and X. X. Zhu, &#8221;<a class="paper" href="">Joint and progressive subspace analysis (JPSA) with spatial-spectral manifold alignment for semi-supervised hyperspectral dimensionality reduction</a>,&#8221; <em>IEEE Transactions on Cybernetics</em>, (accepted for publication), 2020.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Conventional nonlinear subspace learning techniques (e.g., manifold learning) usually introduce some drawbacks in explainability (explicit mapping) and cost-effectiveness (linearization), generalization capability (out-of-sample), and representability (spatial-spectral discrimination). To overcome these shortcomings, a novel linearized subspace analysis technique with spatial-spectral manifold alignment is developed for a semi-supervised hyperspectral dimensionality reduction (HDR), called joint and progressive subspace analysis (JPSA). The JPSA learns a high-level, semantically meaningful, joint spatial-spectral feature representation from hyperspectral data by 1) jointly learning latent subspaces and a linear classifier to find an effective projection direction favorable for classification; 2) progressively searching several intermediate states of subspaces to approach an optimal mapping from the original space to a potential more discriminative subspace; 3) spatially and spectrally aligning manifold structure in each learned latent subspace in order to preserve the same or similar topological property between the compressed data and the original data. A simple but effective classifier, i.e., nearest neighbor (NN), is explored as a potential application for validating the algorithm performance of different HDR approaches. Extensive experiments are conducted to demonstrate the superiority and effectiveness of the proposed JPSA on two widely-used hyperspectral datasets: Indian Pines (92.98\%) and the University of Houston (86.09\%) in comparison with previous state-of-the-art HDR methods. The demo of this basic work (i.e., ECCV2018) is openly available at <a href="https://github.com/danfenghong/ECCV2018_J-Play">https://github.com/danfenghong/ECCV2018_J-Play</a>.</p>
		    </li><br>
		<li>D. Hong, L. Gao, <span class="font700">N. Yokoya</span>, J. Yao, J. Chanussot, Q. Du, and B. Zhang, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/abstract/document/9174822">More diverse means better: Multimodal deep learning meets remote-sensing imagery classification</a>,&#8221; <em>IEEE Transactions on Geoscience and Remote Sensing</em>, (accepted for publication), 2020.<br>
	<a class="file" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9174822"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Classification and identification of the materials lying over or beneath the Earth's surface have long been a fundamental but challenging research topic in geoscience and remote sensing (RS) and have garnered a growing concern owing to the recent advancements of deep learning techniques. Although deep networks have been successfully applied in single-modality-dominated classification tasks, yet their performance inevitably meets the bottleneck in complex scenes that need to be finely classified, due to the limitation of information diversity. In this work, we provide a baseline solution to the aforementioned difficulty by developing a general multimodal deep learning (MDL) framework. In particular, we also investigate a special case of multi-modality learning (MML) -- cross-modality learning (CML) that exists widely in RS image classification applications. By focusing on ``what'', ``where'', and ``how'' to fuse, we show different fusion strategies as well as how to train deep networks and build the network architecture. Specifically, five fusion architectures are introduced and developed, further being unified in our MDL framework. More significantly, our framework is not only limited to pixel-wise classification tasks but also applicable to spatial information modeling with convolutional neural networks (CNNs). To validate the effectiveness and superiority of the MDL framework, extensive experiments related to the settings of MML and CML are conducted on two different multimodal RS datasets. Furthermore, the codes and datasets will be available at: <a href="https://github.com/danfenghong/IEEE_TGRS_MDL-RS">https://github.com/danfenghong/IEEE_TGRS_MDL-RS</a>, contributing to the RS community.</p>
		    </li><br>
		<li>D. Hong, <span class="font700">N. Yokoya</span>, G.-S. Xia, J. Chanussot, and X. X. Zhu, &#8221;<a class="paper" href="">X-ModalNet: A semi-supervised deep cross-modal network for classification of remote sensing data</a>,&#8221; <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, (accepted for publication), 2020.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> This paper addresses the problem of semi-supervised transfer learning with limited cross-modality data in remote sensing. A large amount of multi-modal earth observation images, such as multispectral imagery (MSI) or synthetic aperture radar (SAR) data, are openly available on a global scale, enabling parsing global urban scenes through remote sensing imagery. However, their ability in identifying materials (pixel-wise classification) remains limited, due to the noisy collection environment and poor discriminative information as well as a limited number of well-annotated training images. To this end, we propose a novel cross-modal deep-learning framework, called X-ModalNet, with three well-designed modules: self-adversarial module, interactive learning module, and label propagation module, by learning to transfer more discriminative information from a small-scale hyperspectral image (HSI) into the classification task using a large-scale MSI or SAR data. Significantly, X-ModalNet generalizes well, owing to propagating labels on an updatable graph constructed by high-level features on the top of the network, yielding semi-supervised cross-modality learning. We evaluate X-ModalNet on two multi-modal remote sensing datasets (HSI-MSI and HSI-SAR) and achieve a significant improvement in comparison with several state-of-the-art methods.</p>
		    </li><br>
		<li>E. Mas, R. Paulik, K. Pakoksung, B. Adriano, L. Moya, A. Suppasri, A. Muhari, R. Khomarudin, <span class="font700">N. Yokoya</span>, M. Matsuoka, and S. Koshimura, &#8221;<a class="paper" href="https://link.springer.com/article/10.1007/s00024-020-02501-4">Characteristics of tsunami fragility functions developed using different sources of damage data from the 2018 Sulawesi earthquake and tsunami</a>,&#8221; <em>Pure and Applied Geophysics</em>, 2020.<br>
		<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> We developed tsunami fragility functions using three sources of damage data from the 2018 Sulawesi tsunami at Palu Bay in Indonesia obtained from (i) field survey data (FS), (ii) a visual interpretation of optical satellite images (VI), and (iii) a machine learning and remote sensing approach utilized on multisensor and multitemporal satellite images (MLRS). Tsunami fragility functions are cumulative distribution functions that express the probability of a structure reaching or exceeding a particular damage state in response to a specific tsunami intensity measure, in this case obtained from the interpolation of multiple surveyed points of tsunami flow depth. We observed that the FS approach led to a more consistent function than that of the VI and MLRS methods. In particular, an initial damage probability observed at zero inundation depth in the latter two methods revealed the effects of misclassifications on tsunami fragility functions derived from VI data; however, it also highlighted the remarkable advantages of MLRS methods. The reasons and insights used to overcome such limitations are discussed together with the pros and cons of each method. The results show that the tsunami damage observed in the 2018 Sulawesi event in Indonesia, expressed in the fragility function developed herein, is similar in shape to the function developed after the 1993 Hokkaido Nansei-oki tsunami, albeit with a slightly lower damage probability between zero-to-five-meter inundation depths. On the other hand, in comparison with the fragility function developed after the 2004 Indian Ocean tsunami in Banda Aceh, the characteristics of Palu structures exhibit higher fragility in response to tsunamis. The two-meter inundation depth exhibited nearly 20% probability of damage in the case of Banda Aceh, while the probability of damage was close to 70% at the same depth in Palu.</p>
		    </li><br>
		<li>M. E. Paoletti, J. M. Haut, P. Ghamisi, <span class="font700">N. Yokoya</span>, J. Plaza, and A. Plaza, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/9096509">U-IMG2DSM: Unpaired simulation of digital surface models with generative adversarial networks</a>,&#8221; <em>IEEE Geoscience and Remote Sensing Letters</em>, (Early Access), pp. 1-5, 2020.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="file" href="https://github.com/mhaut/UIMG2DSM" target="_blank"><i class="far fa-file-code"></i> Code </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> High-resolution digital surface models (DSMs) provide valuable height information about the Earth's surface, which can be successfully combined with other types of remotely sensed data in a wide range of applications. However, the acquisition of DSMs with high spatial resolution is extremely time-consuming and expensive with their estimation from a single optical image being an ill-possed problem. To overcome these limitations, this letter presents a new unpaired approach to obtain DSMs from optical images using deep learning techniques. Specifically, our new deep neural model is based on variational autoencoders (VAEs) and generative adversarial networks (GANs) to perform image-to-image translation, obtaining DSMs from optical images. Our newly proposed method has been tested in terms of photographic interpretation, reconstruction error, and classification accuracy using three well-known remotely sensed data sets with very high spatial resolution (obtained over Potsdam, Vaihingen, and Stockholm). Our experimental results demonstrate that the proposed approach obtains satisfactory reconstruction rates that allow enhancing the classification results for these images. The source code of our method is available from: https://github.com/mhaut/UIMG2DSM.</p>
		    </li><br>
		<li>Y. Chen, T.-Z. Huang, W. He, <span class="font700">N. Yokoya</span>, and X.-L. Zhao, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/9096509">Hyperspectral image compressive sensing reconstruction using subspace-based nonlocal tensor ring decomposition</a>,&#8221; <em>IEEE Transactions on Image Processing</em>, (Early Access), pp. 1-16, 2020.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Hyperspectral image compressive sensing reconstruction (HSI-CSR) can largely reduce the high expense and low efficiency of transmitting HSI to ground stations by storing a few compressive measurements, but how to precisely reconstruct the HSI from a few compressive measurements is a challenging issue. It has been proven that considering the global spectral correlation, spatial structure, and nonlocal selfsimilarity priors of HSI can achieve satisfactory reconstruction performances. However, most of the existing methods cannot simultaneously capture the mentioned priors and directly design the regularization term to the HSI. In this article, we propose a novel subspace-based nonlocal tensor ring decomposition method (SNLTR) for HSI-CSR. Instead of designing the regularization of the low-rank approximation to the HSI, we assume that the HSI lies in a low-dimensional subspace. Moreover, to explore the nonlocal self-similarity and preserve the spatial structure of HSI, we introduce a nonlocal tensor ring decomposition strategy to constrain the related coefficient image, which can decrease the computational cost compared to the methods that directly employ the nonlocal regularization to HSI. Finally, a well-known alternating minimization method is designed to efficiently solve the proposed SNLTR. Extensive experimental results demonstrate that our SNLTR method can significantly outperform existing approaches for HSI-CSR.</p>
		    </li><br>
		<li>G. Baier, W. He, and <span class="font700">N. Yokoya</span>, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/9079477">Robust nonlocal low-rank SAR time series despeckling considering speckle correlation by total variation regularization</a>,&#8221; <em>IEEE Transactions on Geoscience and Remote Sensing</em>, accepted for publication, 2020.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="file" href="https://github.com/gbaier/nllrtv" target="_blank"><i class="far fa-file-code"></i> Code </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Outliers and speckle both corrupt time series of sar acquisitions. Owing to the coherence between sar acquisitions, their speckle can no longer be regarded as independent. In this study, we propose an algorithm for nonlocal low-rank time series despeckling, which is robust against outliers and also specifically addresses speckle correlation between acquisitions. By imposing total variation regularization on the signal's speckle component, the correlation between acquisitions can be identified, facilitating the extraction of outliers from unfiltered signals and the correlated speckle. This robustness against outliers also addresses matching errors and inaccuracies in the nonlocal similarity search. Such errors include mismatched data in the nonlocal estimation process, which degrade the denoising performance of conventional similarity-based filtering approaches. Multiple experiments on real and synthetic data assess the performance of the approach by comparing it with state-of-the-art methods. It provides filtering results of comparable quality but is not adversely affected by outliers. The source code is available at https://github.com/gbaier/nllrtv.</p>
		    </li><br>
		<li>C. Yoo, J. Im, D. Cho, <span class="font700">N. Yokoya</span>, J. Xia, and B. Bechtel, &#8221;<a class="paper" href="https://www.mdpi.com/2072-4292/12/9/1398">Estimation of all-weather 1km MODIS land surface temperature for humid summer days</a>,&#8221; <em>Remote Sensing</em>, vol. 12, p. 1398, 2020.<br>
	<a class="file" href="https://www.mdpi.com/2072-4292/12/9/1398/pdf"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Land surface temperature (LST) is used as a critical indicator for various environmental issues because it links land surface fluxes with the surface atmosphere. Moderate-resolution imaging spectroradiometers (MODIS) 1 km LSTs have been widely utilized but have the serious limitation of not being provided under cloudy weather conditions. In this study, we propose two schemes to estimate all-weather 1 km Aqua MODIS daytime (1:30 p.m.) and nighttime (1:30 a.m.) LSTs in South Korea for humid summer days. Scheme 1(S1) is a two-step approach that first estimates 10 km LSTs and then conducts the spatial downscaling of LSTs from 10 km to 1 km. Scheme 2(S2), a one-step algorithm, directly estimates the 1 km all-weather LSTs. Eight advanced microwave scanning radiometer 2 (AMSR2) brightness temperatures, three MODIS-based annual cycle parameters, and six auxiliary variables were used for the LST estimation based on random forest machine learning. To confirm the effectiveness of each scheme, we have performed different validation experiments using clear-sky MODIS LSTs. Moreover, we have validated all-weather LSTs using bias-corrected LSTs from 10 in situ stations. In clear-sky daytime, the performance of S2 was better than S1. However, in cloudy sky daytime, S1 simulated low LSTs better than S2, with an average root mean squared error (RMSE) of 2.6 °C compared to an average RMSE of 3.8 °C over 10 stations. At nighttime, S1 and S2 demonstrated no significant difference in performance both under clear and cloudy sky conditions. When the two schemes were combined, the proposed all-weather LSTs resulted in an average R2 of 0.82 and 0.74 and with RMSE of 2.5 °C and 1.4 °C for daytime and nighttime, respectively, compared to the in situ data. This paper demonstrates the ability of the two different schemes to produce all-weather dynamic LSTs. The strategy proposed in this study can improve the applicability of LSTs in a variety of research and practical fields, particularly for areas that are very frequently covered with clouds.</p>
		    </li><br>
		<li>T.D. Pham, <span class="font700">N. Yokoya</span>, J. Xia, N.T. Ha, N.N. Le, T.T.T. Nguyen, T.H. Dao, T.T.P. Vu, T.D. Pham, and W. Takeuchi, &#8221;<a class="paper" href="https://www.mdpi.com/2072-4292/12/8/1334">Comparison of machine learning methods for estimating mangrove above-ground biomass using multiple source remote sensing data in the red river delta biosphere reserve, Vietnam</a>,&#8221; <em>Remote Sensing</em>, vol. 12, p. 1334, 2020.<br>
	<a class="file" href="https://www.mdpi.com/2072-4292/12/8/1334/pdf"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> This study proposes a hybrid intelligence approach based on an extreme gradient boosting regression and genetic algorithm, namely, the XGBR-GA model, incorporating Sentinel-2, Sentinel-1, and ALOS-2 PALSAR-2 data to estimate the mangrove above-ground biomass (AGB), including small and shrub mangrove patches in the Red River Delta biosphere reserve across the northern coast of Vietnam. We used the novel extreme gradient boosting decision tree (XGBR) technique together with genetic algorithm (GA) optimization for feature selection to construct and verify a mangrove AGB model using data from a field survey of 105 sampling plots conducted in November and December of 2018 and incorporated the dual polarimetric (HH and HV) data of the ALOS-2 PALSAR-2 L-band and the Sentinel-2 multispectral data combined with Sentinel-1 (C-band VV and VH) data. We employed the root-mean-square error (RMSE) and coefficient of determination (R2) to evaluate the performance of the proposed model. The capability of the XGBR-GA model was assessed via a comparison with other machine-learning (ML) techniques, i.e., the CatBoost regression (CBR), gradient boosted regression tree (GBRT), support vector regression (SVR), and random forest regression (RFR) models. The XGBR-GA model yielded a promising result (R2 = 0.683, RMSE = 25.08 Mg·ha−1) and outperformed the four other ML models. The XGBR-GA model retrieved a mangrove AGB ranging from 17 Mg·ha−1 to 142 Mg·ha−1 (with an average of 72.47 Mg·ha−1). Therefore, multisource optical and synthetic aperture radar (SAR) combined with the XGBR-GA model can be used to estimate the mangrove AGB in North Vietnam. The effectiveness of the proposed method needs to be further tested and compared to other mangrove ecosystems in the tropics.</p>
		    </li><br>
		<li>J. Kang, D. Hong, J. Liu, G. Baier, <span class="font700">N. Yokoya</span>, and B. Demir, &#8221;<a class="paper" href="">Learning convolutional sparse coding on complex domain for interferometric phase restoration</a>,&#8221; <em>IEEE Transactions on Neural Networks and Learning Systems</em>, accepted for publication, 2020.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp; <a class="file" href="https://github.com/jiankang1991/ComCSC" target="_blank"><i class="far fa-file-code"></i> Code </a>&nbsp;&nbsp; <a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> </p>
		    </li><br>
		<li>L. Moya , A. Muhari, B. Adriano, S. Koshimura, E. Mas, L. R. M. Perezd, and <span class="font700">N. Yokoya</span>, &#8221;<a class="paper" href="">Detecting urban changes using phase correlation and l1-based sparse model for early disaster response: A case study of the 2018 Sulawesi Indonesia earthquake-tsunami</a>,&#8221; <em>Remote Sensing of Environment</em>, accepted for publication, 2020.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> </p>
		    </li><br>
		<li>T. D. Pham, N. N. Le, N. T. Ha, L. V. Nguyen, J. Xia, <span class="font700">N. Yokoya</span>, T. T. To, H. X. Trinh, L. Q. Kieu, and W. Takeuchi, &#8221;<a class="paper" href="https://www.mdpi.com/2072-4292/12/5/777">Estimating mangrove above-ground biomass using extreme gradient boosting decision trees algorithm with a fusion of Sentinel-2 and ALOS-2 PALSAR-2 data in Can Gio Biosphere Reserve, Vietnam</a>,&#8221; <em>Remote Sensing</em>, vol. 12, no. 5, pp. 777, 2020.<br>
	<a class="file" href="https://www.mdpi.com/2072-4292/12/5/777/pdf"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> This study investigates the effectiveness of gradient boosting decision trees techniques in estimating mangrove above-ground biomass (AGB) at the Can Gio biosphere reserve (Vietnam). For this purpose, we employed a novel gradient-boosting regression technique called the extreme gradient boosting regression (XGBR) algorithm implemented and verified a mangrove AGB model using data from a field survey of 121 sampling plots conducted during the dry season. The dataset fuses the data of the Sentinel-2 multispectral instrument (MSI) and the dual polarimetric (HH, HV) data of ALOS-2 PALSAR-2. The performance standards of the proposed model (root-mean-square error (RMSE) and coefficient of determination (R2)) were compared with those of other machine learning techniques, namely gradient boosting regression (GBR), support vector regression (SVR), Gaussian process regression (GPR), and random forests regression (RFR). The XGBR model obtained a promising result with R2 = 0.805, RMSE = 28.13 Mg ha−1, and the model yielded the highest predictive performance among the five machine learning models. In the XGBR model, the estimated mangrove AGB ranged from 11 to 293 Mg ha−1 (average = 106.93 Mg ha−1). This work demonstrates that XGBR with the combined Sentinel-2 and ALOS-2 PALSAR-2 data can accurately estimate the mangrove AGB in the Can Gio biosphere reserve. The general applicability of the XGBR model combined with multiple sourced optical and SAR data should be further tested and compared in a large-scale study of forest AGBs in different geographical and climatic ecosystems.</p>
		    </li><br>
		<li>B. Adriano, <span class="font700">N. Yokoya</span>, H. Miura, M. Matsuoka, and S. Koshimura, &#8221;<a class="paper" href="https://www.mdpi.com/2072-4292/12/3/561">A semiautomatic pixel-object method for detecting landslides using multitemporal ALOS-2 intensity images</a>,&#8221; <em>Remote Sensing</em>, vol. 12, no. 3, pp. 561, 2020.<br>
	<a class="file" href="https://www.mdpi.com/2072-4292/12/3/561/pdf"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> The rapid and accurate mapping of large-scale landslides and other mass movement disasters is crucial for prompt disaster response efforts and immediate recovery planning. As such, remote sensing information, especially from synthetic aperture radar (SAR) sensors, has significant advantages over cloud-covered optical imagery and conventional field survey campaigns. In this work, we introduced an integrated pixel-object image analysis framework for landslide recognition using SAR data. The robustness of our proposed methodology was demonstrated by mapping two different source-induced landslide events, namely, the debris flows following the torrential rainfall that fell over Hiroshima, Japan, in early July 2018 and the coseismic landslide that followed the 2018 Mw6.7 Hokkaido earthquake. For both events, only a pair of SAR images acquired before and after each disaster by the Advanced Land Observing Satellite-2 (ALOS-2) was used. Additional information, such as digital elevation model (DEM) and land cover information, was employed only to constrain the damage detected in the affected areas. We verified the accuracy of our method by comparing it with the available reference data. The detection results showed an acceptable correlation with the reference data in terms of the locations of damage. Numerical evaluations indicated that our methodology could detect landslides with an accuracy exceeding 80%. In addition, the kappa coefficients for the Hiroshima and Hokkaido events were 0.30 and 0.47, respectively.</p>
		    </li><br>
		<li>T. Uezato, <span class="font700">N. Yokoya</span>, and W. He, &#8221;<a class="paper" href="">Illumination invariant hyperspectral image unmixing based on a digital surface model</a>,&#8221; <em>IEEE Transactions on Image Processing</em>, accepted for publication, 2019.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Although many spectral unmixing models have been developed to address spectral variability caused by variable incident illuminations, the mechanism of the spectral variability is still unclear. This paper proposes an unmixing model, named illumination invariant spectral unmixing (IISU). IISU makes the first attempt to use the radiance hyperspectral data and a LiDAR-derived digital surface model (DSM) in order to physically explain variable illuminations and shadows in the unmixing framework. Incident angles, sky factors, visibility from the sun derived from the LiDAR-derived DSM support the explicit explanation of endmember variability in the unmixing process from radiance perspective. The proposed model was efficiently solved by a straightforward optimization procedure. The unmixing results showed that the other state-of-the-art unmixing models did not work well especially in the shaded pixels. On the other hand, the proposed model estimated more accurate abundances and shadow compensated reflectance than the existing models.</p>
		    </li><br>
		<li>D. Hong, X. Wu, P. Ghamisi, J. Chanussot, <span class="font700">N. Yokoya</span>, and X. X. Zhu, &#8221;<a class="paper" href="">Invariant attribute profiles: A spatial-frequency joint feature extractor for hyperspectral image classification</a>,&#8221; <em>IEEE Trans. Geosci. Remote Sens.</em>, accepted for publication, 2019.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span></p>
		    </li><br>
		<li>Y. Chen, L. Huang, L. Zhu, <span class="font700">N. Yokoya</span>, and X. Jia, &#8221;<a class="paper" href="">Fine-grained classification of hyperspectral imagery based on deep learning</a>,&#8221; <em>Remote Sensing</em>, accepted for publication, 2019.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span></p>
		    </li><br>
		<li>Y. Chen, W. He, <span class="font700">N. Yokoya</span>, and T.-Z. Huang, &#8221;<a class="paper" href="">Non-local tensor ring decomposition for hyperspectral image denoising</a>,&#8221; <em>IEEE Trans. Geosci. Remote Sens.</em>, accepted for publication, 2019.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Hyperspectral image (HSI) denoising is a fundamental problem in remote sensing and image processing. Recently, non-local low-rank tensor approximation based denoising methods have attracted much attention, due to the advantage of fully exploiting the non-local self-similarity and global spectral correlation. Existing non-local low-rank tensor approximation methods were mainly based on two common Tucker or CP decomposition and achieved the state-of-the-art results, but they suffer some troubles and are not the best approximation for a tensor. For example, the number of parameters of Tucker decomposition increases exponentially follow its dimension, and CP decomposition cannot better preserve the intrinsic correlation of HSI. In this paper, we propose a non-local tensor ring (TR) approximation for HSI denoising by utilizing TR decomposition to simultaneously explore non-local self-similarity and global spectral low-rank characteristic. TR decomposition approximates a high-order tensor as a sequence of cyclically contracted three-order tensors, which has a strong ability to explore these two intrinsic priors and improve the HSI denoising result. Moreover, we develop an efficient proximal alternating minimization algorithm to efficiently optimize the proposed TR decomposition model. Extensive experiments on three simulated datasets under several noise levels and two real datasets testify that the proposed TR model performs better HSI denoising results than several state-of-the-art methods in term of quantitative and visual performance evaluations.</p>
		    </li><br>
		<li>D. Hong, <span class="font700">N. Yokoya</span>, J. Chanussot, J. Xu, and X. X. Zhu, &#8221;<a class="paper" href="">Learning to propagate labels on graphs: An iterative multitask regression framework for semi-supervised hyperspectral dimensionality reduction</a>,&#8221; <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, accepted for publication, 2019.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Hyperspectral dimensionality reduction (HDR), an important preprocessing step prior to high-level data analysis, has been garnering growing attention in the remote sensing community. Although a variety of methods, both unsupervised and supervised models, have been proposed for this task, yet the discriminative ability in feature representation still remains limited due to the lack of a powerful tool that effectively exploits the labeled and unlabeled data in the HDR process. A semi-supervised HDR approach, called iterative multitask regression (IMR), is proposed in this paper to address this need. IMR aims at learning a low-dimensional subspace by jointly considering the labeled and unlabeled data, and also bridging the learned subspace with two regression tasks: labels and pseudo-labels initialized by a given classifier. More significantly, IMR dynamically propagates the labels on a learnable graph and progressively refines pseudo-labels, yielding a well-conditioned feedback system. Experiments conducted on three widely-used hyperspectral image datasets demonstrate that the dimension-reduced features learned by the proposed IMR framework with respect to classification or recognition accuracy are superior to those of related state-of-the-art HDR approaches.</p>
		    </li><br>
		<li>D. Hong, J. Chanussot, <span class="font700">N. Yokoya</span>, J. Kang, and X. X. Zhu, &#8221;<a class="paper" href="">Learning shared cross-modality representation using multispectral-LiDAR and hyperspectral data</a>,&#8221; <em>IEEE Geosci. Remote Sens. Lett.</em>, accepted for publication, 2019.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Due to the ever-growing diversity of the data source, multi-modality feature learning has attracted more and more attention. However, most of these methods are designed by jointly learning feature representation from multi-modalities that exist in both training and test sets, yet they are less investigated in absence of certain modality in the test phase. To this end, in this letter, we propose to learn a shared feature space across multi-modalities in the training process. By this way, the out-of-sample from any of multi-modalities can be directly projected onto the learned space for a more effective cross-modality representation. More significantly, the shared space is regarded as a latent subspace in our proposed method, which connects the original multi-modal samples with label information to further improve the feature discrimination. Experiments are conducted on the multispectral-Lidar and hyperspectral dataset provided by the 2018 IEEE GRSS Data Fusion Contest to demonstrate the effectiveness and superiority of the proposed method in comparison with several popular baselines.</p>
		    </li><br>
		<li>Y. Chen, W. He, <span class="font700">N. Yokoya</span>, and T.-Z. Huang, &#8221;<a class="paper" href="">Blind cloud and cloud shadow removal of multitemporal images based on total variation regularized low-rank sparsity decomposition</a>,&#8221; <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, (accepted for publication), 2019.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Cloud and cloud shadow (cloud/shadow) removal from multitemporal satellite images is a challenging task and has elicited much attention for subsequent information extraction. Regarding cloud/shadow areas as missing information, low-rank matrix/tensor completion based methods are popular to recover information undergoing cloud/shadow degradation. However, existing methods required to determine the cloud/shadow locations in advance and failed to completely use the latent information in cloud/shadow areas. In this study, we propose a blind cloud/shadow removal method for time-series remote sensing images by unifying cloud/shadow detection and removal together. First, we decompose the degraded image into low-rank clean image (surface-reflected) component and sparse (cloud/shadow) component, which can simultaneously and completely use the underlying characteristics of these two components. Meanwhile, the spatial-spectral total variation regularization is introduced to promote the spatial-spectral continuity of the cloud/shadow component. Second, the cloud/shadow locations are detected from the sparse component using a threshold method. Finally, we adopt the cloud/shadow detection results to guide the information compensation from the original observed images to better preserve the information in cloud/shadow-free locations. The problem of the proposed model is efficiently addressed using the alternating direction method of multipliers. Both simulated and real datasets are performed to demonstrate the effectiveness of our method for cloud/shadow detection and removal when compared with other state-of-the-art methods.</p>
		    </li><br>
		<li>Y. Chen, W. He, <span class="font700">N. Yokoya</span>, and T.-Z. Huang, &#8221;<a class="paper" href="">Hyperspectral image restoration using weighted group sparsity regularized low-rank tensor decomposition</a>,&#8221; <em>IEEE Trans. Cybernetics</em>, (accepted for publication), 2019.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Mixed noise (such as Gaussian, impulse, stripe, and deadline noises) contamination is a common phenomenon in hyperspectral imagery (HSI), greatly degrading visual quality and affecting subsequent processing accuracy. By encoding sparse prior to the spatial or spectral difference images, total variation (TV) regularization is an efficient tool for removing the noises. However, the previous TV term cannot maintain the shared group sparsity pattern of the spatial difference images of different spectral bands. To address this issue, this study proposes a group sparsity regularization of the spatial difference images for HSI restoration. Instead of using L1 or L2-norm (sparsity) on the difference image itself, we introduce a weighted L2,1-norm to constrain the spatial difference image cube, efficiently exploring the shared group sparse pattern. Moreover, we employ the well-known low-rank Tucker decomposition to capture the global spatial-spectral correlation from three HSI dimensions. To summarize, a weighted group sparsity regularized low-rank tensor decomposition (LRTDGS) method is presented for HSI restoration. An efficient augmented Lagrange multiplier algorithm is employed to solve the LRTDGS model. The superiority of this method for HSI restoration is demonstrated by a series of experimental results from both simulated and real data, as compared to other state-of-the-art TV regularized low-rank matrix/tensor decomposition methods.</p>
		    </li><br>
		<li>W. He, <span class="font700">N. Yokoya</span>, L. Yuan, and Q. Zhao, &#8221;<a class="paper" href="">Remote sensing image reconstruction using tensor ring completion and total-variation</a>,&#8221; <em>IEEE Trans. Geosci. Remote Sens.</em>, (accepted for publication), 2019.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span>Time-series remote sensing (RS) images are often corrupted by various types of missing information such as dead pixels, clouds, and cloud shadows that significantly influence the subsequent applications. In this paper, we introduce a new low-rank tensor decomposition model, termed tensor ring (TR) decomposition, to the analysis of RS datasets and propose a TR completion method for the missing information reconstruction. The proposed TR completion model has the ability to utilize the low-rank property of time-series RS images from different dimensions. To furtherly explore the smoothness of the RS image spatial information, total-variation regularization is also incorporated into the TR completion model. The proposed model is efficiently solved using two algorithms, the augmented Lagrange multiplier (ALM) and the alternating least square (ALS) methods. The simulated and real data experiments show superior performance compared to other state-of-the-art low-rank related algorithms.</p>
		    </li><br>
		<li>Y. Xu, B. Du, L. Zhang, D. Cerra, M. Pato, E. Carmona, S. Prasad, <span class="font700">N. Yokoya</span>, R. Hansch, and B. Le Saux, &#8221;<a class="paper" href="https://ieeexplore.ieee.org/document/8727489">Advanced multi-sensor optical remote sensing for urban land use and land cover classification: Outcome of the 2018 IEEE GRSS Data Fusion Contest</a>,&#8221; <em>IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.</em>, (accepted for publication), 2019.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span></p>
		    </li><br>
		<li>B. Adriano, J. Xia, G. Baier, <span class="font700">N. Yokoya</span>, S. Koshimura, &#8221;<a class="paper" href="https://www.mdpi.com/2072-4292/11/7/886">Multi-source data fusion based on ensemble learning for rapid building damage mapping during the 2018 Sulawesi Earthquake and Tsunami in Palu, Indonesia</a>,&#8221; <em>Remote Sensing</em>, vol. 11, no. 7, p. 886, 2019.<br>
	<a class="file" href="https://www.mdpi.com/2072-4292/11/7/886/pdf"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> This work presents a detailed analysis of building damage recognition, employing multi-source data fusion and ensemble learning algorithms for rapid damage mapping tasks. A damage classification framework is introduced and tested to categorize the building damage following the recent 2018 Sulawesi earthquake and tsunami. Three robust ensemble learning classifiers were investigated for recognizing building damage from SAR and optical remote sensing datasets and their derived features. The contribution of each feature dataset was also explored, considering different combinations of sensors as well as their temporal information. SAR scenes acquired by the ALOS-2 PALSAR-2 and Sentinel-1 sensors were used. The optical Sentinel-2 and PlanetScope sensors were also included in this study. A non-local filter in the preprocessing phase was used to enhance the SAR features. Our results demonstrated that the canonical correlation forests classifier performs better in comparison to the other classifiers. In the data fusion analysis, DEM- and SAR-derived features contributed the most in the overall damage classification. Our proposed mapping framework successfully classifies four levels of building damage (with overall accuracy > 90%, average accuracy > 67%). The proposed framework learned the damage patterns from a limited available human-interpreted building damage annotation and expands this information to map a larger affected area. This process including pre- and post-processing phases were completed in about 3 hours after acquiring all raw datasets.</p>
		    </li><br>
		<li>P. Ghamisi, B. Rasti, <span class="font700">N. Yokoya</span>, Q. Wang, B. Höfle, L. Bruzzone, F. Bovolo, M. Chi, K. Anders, R. Gloaguen, P. M. Atkinson, and J. A. Benedikt, &#8221;<a class="paper" href="">Multisource and multitemporal data fusion in remote sensing</a>,&#8221; <em>IEEE Geoscience and Remote Sensing Magazine</em>, vol. 7, no. 1, pp. 6-39, 2019.<br>
	<a class="file" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8672156"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> The sharp and recent increase in the availability of data captured by different sensors combined with their considerably heterogeneous natures poses a serious challenge for the effective and efficient processing of remotely sensed data. Such an increase in remote sensing and ancillary datasets, however, opens up the possibility of utilizing multimodal datasets in a joint manner to further improve the performance of the processing approaches with respect to the application at hand. Multisource data fusion has, therefore, received enormous attention from researchers worldwide for a wide variety of applications. Moreover, thanks to the revisit capability of several spaceborne sensors, the integration of the temporal information with the spatial and/or spectral/backscattering information of the remotely sensed data is possible and helps to move from a representation of 2D/3D data to 4D data structures, where the time variable adds new information as well as challenges for the information extraction algorithms. There are a huge number of research works dedicated to multisource and multitemporal data fusion, but the methods for the fusion of different modalities have expanded in different paths according to each research community. This paper brings together the advances of multisource and multitemporal data fusion approaches with respect to different research communities and provides a thorough and discipline-specific starting point for researchers at different levels (i.e., students, researchers, and senior researchers) willing to conduct novel investigations on this challenging topic by supplying sufficient detail and references. More specifically, this paper provides a bird's-eye view of many important contributions specifically dedicated to the topics of pansharpening and resolution enhancement, point cloud data fusion, hyperspectral and LiDAR data fusion, multitemporal data fusion, as well as big data and social media. In addition, the main challenges and possible future research for each section are outlined and discussed.</p>
		    </li><br>
		<li>T. D. Pham, <span class="font700">N. Yokoya</span>, D. T. Bui, K. Yoshino, and D. A. Friess, &#8221;<a class="paper" href="">Remote sensing approaches for monitoring mangrove species, structure and biomass: opportunities and challenges</a>,&#8221; <em>Remote Sensing</em>, vol. 11, no. 3, pp. 230, 2019.<br>
	<a class="file" href="https://www.mdpi.com/2072-4292/11/3/230/pdf"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> The mangrove ecosystem plays a vital role in the global carbon cycle, by reducing greenhouse gas emissions and mitigating the impacts of climate change. However, mangroves have been lost worldwide, resulting in substantial carbon stock losses. Additionally, some aspects of the mangrove ecosystem remain poorly characterized compared to other forest ecosystems due to practical difficulties in measuring and monitoring mangrove biomass and their carbon stocks. Without a quantitative method for effectively monitoring biophysical parameters and carbon stocks in mangroves, robust policies and actions for sustainably conserving mangroves in the context of climate change mitigation and adaptation are more difficult. In this context, remote sensing provides an important tool for monitoring mangroves and identifying attributes such as species, biomass, and carbon stocks. A wide range of studies is based on optical imagery (aerial photography, multispectral, and hyperspectral) and synthetic aperture radar (SAR) data. Remote sensing approaches have been proven effective for mapping mangrove species, estimating their biomass, and assessing changes in their extent. This review provides an overview of the techniques that are currently being used to map various attributes of mangroves, summarizes the studies that have been undertaken since 2010 on a variety of remote sensing applications for monitoring mangroves, and addresses the limitations of these studies. We see several key future directions for the potential use of remote sensing techniques combined with machine learning techniques for mapping mangrove areas and species, and evaluating their biomass and carbon stocks.</p>
		    </li><br>
		<li>D. Hong, <span class="font700">N. Yokoya</span>, J. Chanussot, and X. X. Zhu, &#8221;<a class="paper" href="">CoSpace: Common subspace learning from hyperspectral-multispectral correspondences</a>,&#8221; <em>IEEE Trans. Geosci. Remote Sens.</em>, vol. 57, no. 7, pp. 4349-4359, 2019.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> With a large amount of open satellite multispectral imagery (e.g., Sentinel-2 and Landsat-8), considerable attention has been paid to global multispectral land cover classification. However, its limited spectral information hinders further improving the classification performance. Hyperspectral imaging enables discrimination between spectrally similar classes but its swath width from space is narrow compared to multispectral ones. To achieve accurate land cover classification over a large coverage, we propose a cross-modality feature learning framework, called common subspace learning (CoSpace), by jointly considering subspace learning and supervised classification. By locally aligning the manifold structure of the two modalities, CoSpace linearly learns a shared latent subspace from hyperspectral-multispectral (HS-MS) correspondences. The multispectral out-of-samples can be then projected into the subspace, which are expected to take advantages of rich spectral information of the corresponding hyperspectral data used for learning, and thus leads to a better classification. Extensive experiments on two simulated HS-MS datasets (University of Houston and Chikusei), where HS-MS data sets have trade-offs between coverage and spectral resolution, are performed to demonstrate the superiority and effectiveness of the proposed method in comparison with previous state-of-the-art methods.</p>
		    </li><br>
		    <li>D. Hong, <span class="font700">N. Yokoya</span>, N. Ge, J. Chanussot, and X. X. Zhu, &#8221;<a class="paper" href="">Learnable manifold alignment (LeMA) : A semi-supervised cross-modality learning framework for land cover and land use classification</a>,&#8221; <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, vol. 147, pp. 193-205, 2019.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> In this paper, we aim at tackling a general but interesting cross-modality feature learning question in remote sensing community - can a limited amount of highly-discriminative (e.g., hyperspectral) training data improve the performance of a classification task using a large amount of poorly-discriminative (e.g., multispectral) data? Traditional semi-supervised manifold alignment methods do not perform sufficiently well for such problems, since the hyperspectral data is very expensive to be largely collected in a trade-off between time and efficiency, compared to the multispectral data. To this end, we propose a novel semi-supervised cross-modality learning framework, called learnable manifold alignment (LeMA). LeMA learns a joint graph structure directly from the data instead of using a given fixed graph defined by a Gaussian kernel function. With the learned graph, we can further capture the data distribution by graph-based label propagation, which enables finding a more accurate decision boundary. Additionally, an optimization strategy based on the alternating direction method of multipliers (ADMM) is designed to solve the proposed model. Extensive experiments on two hyperspectral-multispectral datasets demonstrate the superiority and effectiveness of the proposed method in comparison with several state-of-the-art methods.</p>
		    </li><br>	
		<li>D. Hong, <span class="font700">N. Yokoya</span>, J. Chanussot, and X. X. Zhu, &#8221;<a class="paper" href="">An augmented linear mixing model to address spectral variability for hyperspectral unmixing</a>,&#8221; <em>IEEE Transactions on Image Processing</em>, vol. 28, no. 4, pp. 1923-1938, 2018.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Hyperspectral imagery collected from airborne or satellite sources inevitably suffers from spectral variability, making it difficult for spectral unmixing to accurately estimate abundance maps. The classical unmixing model, the linear mixing model (LMM), generally fails to handle this sticky issue effectively. To this end, we propose a novel spectral mixture model, called the augmented linear mixing model (ALMM), to address spectral variability by applying a data-driven learning strategy in inverse problems of hyperspectral unmixing. The proposed approach models the main spectral variability (i.e., scaling factors) generated by variations in illumination or typography separately by means of the endmember dictionary. It then models other spectral variabilities caused by environmental conditions (e.g., local temperature and humidity, atmospheric effects) and instrumental configurations (e.g., sensor noise), as well as material nonlinear mixing effects, by introducing a spectral variability dictionary. To effectively run the data-driven learning strategy, we also propose a reasonable prior knowledge for the spectral variability dictionary, whose atoms are assumed to be low-coherent with spectral signatures of endmembers, which leads to a well-known low-coherence dictionary learning problem. Thus, a dictionary learning technique is embedded in the framework of spectral unmixing so that the algorithm can learn the spectral variability dictionary and estimate the abundance maps simultaneously. Extensive experiments on synthetic and real datasets are performed to demonstrate the superiority and effectiveness of the proposed method in comparison with previous state-of-the-art methods.</p>
		    </li><br>
		<li>W. He and <span class="font700">N. Yokoya</span>, &#8221;<a class="paper" href="https://www.mdpi.com/2220-9964/7/10/389">Multi-temporal Sentinel-1 and -2 data fusion for optical image simulation</a>,&#8221; <em>ISPRS International Journal of Geo-Information</em>, vol. 7, no. 10: 389, 2018.<br>
	<a class="file" href="http://www.mdpi.com/2220-9964/7/10/389/pdf"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> In this paper, we present the optical image simulation from synthetic aperture radar (SAR) data using deep learning based methods. Two models, i.e., optical image simulation directly from the SAR data and from multi-temporal SAR-optical data, are proposed to testify the possibilities. The deep learning based methods that we chose to achieve the models are a convolutional neural network (CNN) with a residual architecture and a conditional generative adversarial network (cGAN). We validate our models using the Sentinel-1 and -2 datasets. The experiments demonstrate that the model with multi-temporal SAR-optical data can successfully simulate the optical image, meanwhile, the model with simple SAR data as input failed. The optical image simulation results indicate the possibility of SAR-optical information blending for the subsequent applications such as large-scale cloud removal, and optical data temporal super-resolution. We also investigate the sensitivity of the proposed models against the training samples, and reveal possible future directions.</p>
		    </li><br>	
		<li>L. Guanter, M. Brell, J. C.-W. Chan, C. Giardino, J. Gomez-Dans, C. Mielke, F. Morsdorf, K. Segl, and <span class="font700">N. Yokoya</span>, &#8221;<a class="paper" href="https://link.springer.com/article/10.1007%2Fs10712-018-9485-z">Synergies of spaceborne imaging spectroscopy with other remote sensing approaches</a>,&#8221; <em>Surveys in Geophysics</em>, pp. 1-31, 2018.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Imaging spectroscopy (IS), also commonly known as hyperspectral remote sensing, is a powerful remote sensing technique for the monitoring of the Earth’s surface and atmosphere. Pixels in optical hyperspectral images consist of continuous reflectance spectra formed by hundreds of narrow spectral channels, allowing an accurate representation of the surface composition through spectroscopic techniques. However, technical constraints in the definition of imaging spectrometers make spectral coverage and resolution to be usually traded by spatial resolution and swath width, as opposed to optical multispectral (MS) systems typically designed to maximize spatial and/or temporal resolution. This complementarity suggests that a synergistic exploitation of spaceborne IS and MS data would be an optimal way to fulfill those remote sensing applications requiring not only high spatial and temporal resolution data, but also rich spectral information. On the other hand, IS has been shown to yield a strong synergistic potential with non-optical remote sensing methods, such as thermal infrared (TIR) and light detection and ranging (LiDAR). In this contribution we review theoretical and methodological aspects of potential synergies between optical IS and other remote sensing techniques. The focus is put on the evaluation of synergies between spaceborne optical IS and MS systems because of the expected availability of the two types of data in the next years. Short reviews of potential synergies of IS with TIR and LiDAR measurements are also provided.</p>
		    </li><br>
		<li>T.-Y. Ji, <span class="font700">N. Yokoya</span>, X. X. Zhu, and T.-Z. Huang, &#8221;<a class="paper" href="">Non-local tensor completion for multitemporal remotely sensed images inpainting</a>,&#8221; <em>IEEE Trans. Geosci. Remote Sens.</em>, vol. 56, no. 6, pp. 3047-3061, 2018.<br>
	<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Remotely sensed images may contain some missing areas because of poor weather conditions and sensor failure. Information of those areas may play an important role in the interpretation of multitemporal remotely sensed data. The paper aims at reconstructing the missing information by a non-local low-rank tensor completion method (NL-LRTC). First, non-local correlations in the spatial domain are taken into account by searching and grouping similar image patches in a large search window. Then low-rankness of the identified 4-order tensor groups is promoted to consider their correlations in spatial, spectral, and temporal domains, while reconstructing the underlying patterns. Experimental results on simulated and real data demonstrate that the proposed method is effective both qualitatively and quantitatively. In addition, the proposed method is computationally efficient compared to other patch based methods such as the recent proposed PM-MTGSR method. </p>
		    </li><br>
		<li>J. Xia, <span class="font700">N. Yokoya</span>, and A. Iwasaki, &#8221;<a class="paper" href="">Fusion of hyperspectral and LiDAR data with a novel ensemble classifier</a>,&#8221; <em>IEEE Geosci. Remote Sens. Lett.</em>, vol. 15, no. 6, pp. 957-961, 2018.<br>
		<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> Due to the development of sensors and data acquisition technology, the fusion of features from multiple sensors is a very hot topic. In this letter, the use of morphological features to fuse an HS image and a light detection and ranging (LiDAR)-derived digital surface model (DSM) is exploited via an ensemble classifier. In each iteration, we first apply morphological openings and closings with partial reconstruction on the first few principal components (PCs) of the HS and LiDAR datasets to produce morphological features to model spatial and elevation information for HS and LiDAR datasets. Second, three groups of features (i.e., spectral, morphological features of HS and LiDAR data) are split into several disjoint subsets. Third, data transformation is applied to each subset and the features extracted in each subset are stacked as the input of a random forest (RF) classifier. Three data transformation methods, including principal component analysis (PCA), linearity preserving projection (LPP), and unsupervised graph fusion (UGF) are introduced into the ensemble classification process. Finally, we integrate the classification results achieved at each step by a majority vote. Experimental results on co-registered HS and LiDAR-derived DSM demonstrate the effectiveness and potentialities of the proposed ensemble classifier.</p>
		    </li><br>
		<li>P. Ghamisi and <span class="font700">N. Yokoya</span>, &#8221;<a class="paper" href="http://ieeexplore.ieee.org/document/8306501/">IMG2DSM: Height simulation from single imagery using conditional generative adversarial nets</a>,&#8221; <em>IEEE Geosci. Remote Sens. Lett.</em>, vol. 15, no. 5, pp. 794-798, 2018.<br>
		<a class="file" href=""><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span> This paper proposes a groundbreaking approach in the remote sensing community to simulating digital surface model (DSM) from a single optical image. This novel technique uses conditional generative adversarial nets whose architecture is based on an encoder-decoder network with skip connections (generator) and penalizing structures at the scale of image patches (discriminator). The network is trained on scenes where both DSM and optical data are available to establish an image-to-DSM translation rule. The trained network is then utilized to simulate elevation information on target scenes where no corresponding elevation information exists. The capability of the approach is evaluated both visually (in terms of photo interpretation) and quantitatively (in terms of reconstruction errors and classification accuracies) on sub-decimeter spatial resolution datasets captured over Vaihingen, Potsdam, and Stockholm. The results confirm the promising performance of the proposed framework.</p>
		    </li><br>
		<li><span class="font700">N. Yokoya</span>, P. Ghamisi, J. Xia, S. Sukhanov, R. Heremans, I. Tankoyeu, B. Bechtel, B. Le Saux, G. Moser, and D. Tuia, &#8221;<a class="paper" href="">Open data for global multimodal land use classification: Outcome of the 2017 IEEE GRSS Data Fusion Contest</a>,&#8221; <em>IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.</em>, vol. 11, no. 5, pp. 1363-1377, 2018.<br>
		    </li><br>
    		<li>J. Xia, P. Ghamisi, <span class="font700">N. Yokoya</span>, and A. Iwasaki, &#8221;<a class="paper" href="https://www.researchgate.net/publication/319236274_Random_Forest_Ensembles_and_Extended_Multi-Extinction_Profiles_for_Hyperspectral_Image_Classification">Random forest ensembles and extended multi-extinction profiles for hyperspectral image classification</a>,&#8221; <em>IEEE Trans. Geosci. Remote Sens.</em>, vol. 56, no. 1, pp. 202-216, 2018.<br>
	<a class="file" href="https://www.researchgate.net/profile/Junshi_Xia2/publication/319236274_Random_Forest_Ensembles_and_Extended_Multi-Extinction_Profiles_for_Hyperspectral_Image_Classification/links/599d2731a6fdcc50034cb6aa/Random-Forest-Ensembles-and-Extended-Multi-Extinction-Profiles-for-Hyperspectral-Image-Classification.pdf?_iepl%5BhomeFeedViewId%5D=IhhMThKphbyF8KE0A1MOgt1TomuY07jBCAR0&_iepl%5Bcontexts%5D%5B0%5D=pcfhf&_iepl%5BinteractionType%5D=publicationDownload&origin=publication_detail&ev=pub_int_prw_xdl&msrp=-AkqYVYJy2xpqLbeSM8WJHFFsmH78ZV376WFF2KWn-2-HNEfFsCpxuueGmMwSH08IKAZ_vj1M8F-0IFthzLCRvHBc_oZZPsf3r7glBPM1TqU8MHpNI4bcY1d.Qij1-lZJXILkoms5i9nNkgVDLuLxSaNmjXTV3R3YLDEGDIn_YrZhdXCJQC14a4PAfw41onXE9W0zJnkXfMWl3xNeImP5svzQlYVFBw.e7LagyEU8NLdtoSZS3YctBCgJmKCp1I9TSxw2-gydN5JSDz-_ryoaSCDyPvXMFiAj5DjNcVilH0b-9su6UoU6pYNOZe5kWT12NzydQ.1oAXeoZFlE1DuP5OpWil5UEC3DlFRJjdJ_-6oqBhsDv9Zbny8Dh_nqE_HmJ-i2YkvV837O27sD3-rIPuTF7_uVAn00TEAqnp8fNQTA"><i class="fa fa-file-pdf-o"></i> PDF </a>&nbsp;&nbsp;<a class="abst_bt"><i class="fa fa-newspaper-o"></i> Abstract</a>
		    <p class="abst"><span style="color: #191970;"><span class="font700"><em>Abstract:</em></span></span>
            Classification techniques for hyperspectral images based on random forest (RF) ensembles and extended multi-extinction profiles (EMEPs) are proposed as a means of improving performance. To this end, five strategies-bagging, boosting, random subspace, rotation-based, and boosted rotation-based---are used to construct the RF ensembles. Extinction profiles (EPs), which are based on an extrema-oriented connected filtering technique, are applied to the images associated with the first informative components extracted by independent component analysis, leading to a set of EMEPs. The effectiveness of the proposed method is investigated on two benchmark hyperspectral images, University of Pavia and Indian Pines. Comparative experimental evaluations reveal the superior performance of the proposed methods, especially those employing rotation-based and boosted rotation-based approaches. An additional advantage is that the CPU processing time is acceptable. 
            </p>
		    </li><br>
		</ol>
		
  <hr class="featurette-divider">

  </div><!-- /.container -->


  <!-- FOOTER -->
  <footer class="container">
    <!--<p class="float-end"><a href="#">Back to top</a></p>-->
    <p>&copy; 2023 Geoinformatics Unit</p>
  </footer>
</main>


    <script src="./assets/dist/js/bootstrap.bundle.min.js"></script>

      
  </body>
</html>
